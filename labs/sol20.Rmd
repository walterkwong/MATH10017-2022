---
title: "R Notebook"
output: html_notebook
---

# Lab 16

In machine learning, one common task is [regression analysis](https://en.wikipedia.org/wiki/Regression_analysis): using the input data, we want to predict the output. This is a typical machine learning task: Give students' first year score, I want to predict their final year grades. Given patients physical condition, doctors want to predict their blood pressure. etc.

Today, we will play with an astronomy dataset. It records the relative locations of stars in Galaxy NGC7531. We would like to **use these location information** to predict the **velocity** of stars.

We will practice using data frames in a real-world application.

![](https://upload.wikimedia.org/wikipedia/commons/d/dc/NGC_7531_GALEX_WikiSky.jpg){width="188"}

Galaxy NGC7531, from Wikipedia.

## Load data frame

The data frame has been downloaded and processed for you. Run the following command to load the dataset.

```{r}
set.seed(1)
galaxy <- read.table("galaxy")
```

Let us look at the summary of our dataset:

```{r}
dim(galaxy)
summary(galaxy)
```

The data frame contains **323** observations and **5** columns. All entries are numeric. The last column is the velocity we want to predict. The first four variables (columns) are information used to express the relative positions of stars in a galaxy. More information on the dataset can be found [here](https://hastie.su.domains/ElemStatLearn/datasets/galaxy.info.txt).

## Split Data

Let us split the dataset into two parts: Training and testing. **Our target is using the training dataset to make predictions for each observation in the testing dataset.** For example, in our TB1 project, $X$ and $Y$ matrices are our training dataset and $T$ is the testing dataset (without output).

In this task, the output is `velocity` while the inputs are the other 4 columns.

First, let us randomly select 200 observations from `galaxy` and store them in a new data frame `train_data`.

```{r}

idx <- sample(1:323,200) # This produce a random integer vector, containing random integers sampled from sequence 1 to 323. 
print(idx)

# Your code here:

train_data <- galaxy[idx, ]
```

Now, select the rest of the observations from `galaxy` and store them in a data frame `test_data`:

```{r}

a <- matrix(1:16, 4, 4)

# exclude the 1st and 3rd row from a matrix A:
b <- a[-c(1, 3), ]

# Your code here:
test_data <- galaxy[-idx, ]
```

Now inspect your data frames from the environment pane. Do they have correct dimensions?

## Standardize data

Let us normalize our dataset **inputs** before making predictions.

Write a function which takes one vector `v` as input and returns a standardized vector `s`:

$$
s = (v-\mu)/\sigma, 
$$

where $\mu$ is the mean of `v` and $\sigma$ is the standard deviation of `v`.

```{r}
standardize  <- function(v){
  # Write your code here:
  return ((v - mean(v))/sd(v))

}
```

Now test it on a toy vector:

```{r}
s <- standardize(1:4)
summary(s)
```

Apply the `standardize` function to **the first 4 columns** of `test_data`(the 5th column is the output, we don't want to standardize it!). Assign the result back to `test_data`.

Hint: your code should look like `test_data[, 1:4] <- data.frame(apply(â€¦))`. You need to convert the result from `apply` back to a data frame.

```{r}
# Write your code here:
test_data[, 1:4] <- data.frame(apply(test_data[, 1:4], 2, standardize))
```

Check the mean and standard deviation of each columns of `test_data`. Have they been successfully standardized?

```{r}
summary(test_data)
```

Similarly, apply the `standardize` function to **the first 4 columns** of `train_data`.

```{r}
# Write your code here:
train_data[, 1:4] <- data.frame(apply(train_data[,1:4], 2, standardize))
```

```{r}
summary(train_data)
```

## Predict!

Let us first introduce a prediction function. It predicts the output given a single input vector.

The following function takes a four dimensional input vector `x`, and produce a prediction of the velocity (can you tell how it works? see lecture slides).

```{r}
predict <- function(x){
  dist <- c()
  # find the 5 nearest neighbours of x, 
  for(i in 1:dim(train_data)[1]){
    dist[i] <- sqrt(sum((x - train_data[i,-5])^2))
  }

  nei <- order(dist)[1:5]
  # average their output as the prediction
  pred <- mean(train_data[nei,5])
  
  return(pred)
}
```

Apply this function to **all the observations (except the 5th column)** in `test_data`. Store the result to a new column `pred` in `test_data`.

```{r}
a <- data.frame(c(1,2,3))
# adding the second column "newcol" to a
a$newcol <- c(4,5,6)

# Write your code here:
test_data$pred <- apply(test_data[, 1:4], 1, predict)
```

Compute [the mean squared error](https://en.wikipedia.org/wiki/Mean_squared_error) of your prediction. This is the numerical quantification of how good your predictions are (the lower the better).

```{r}
# Write your code here
mean((test_data$pred - test_data$velocity)^2)
```

Is it any good?

Now consider a naive prediction method: I take the average of the training output `train_data$velocity` , and use it as the prediction of the velocity of the testing dataset. What is the mean squared error of this naive approach?

```{r}
# Write your code here (one line)
mean((mean(train_data$velocity) - test_data$velocity)^2)
```

How much better our prediction method is than the naive approach in terms of mean squared error?
